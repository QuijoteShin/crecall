# Gu√≠a de Selecci√≥n de Modelos

## Clasificadores (Intent Classification)

### M√©tricas Clave a Buscar en HuggingFace:

1. **Parameters** (Par√°metros del modelo)
   - üéØ Objetivo: **<100M** par√°metros
   - ‚úÖ √ìptimo: 20M-80M (distil*, deberta-v3-xsmall)
   - ‚ö†Ô∏è Evitar: >200M (muy lento en GPU <8GB)

2. **Model Size** (Tama√±o en disco)
   - üéØ Objetivo: **<500MB**
   - ‚úÖ √ìptimo: 200-400MB
   - ‚ö†Ô∏è Evitar: >1GB (carga lenta, ocupa mucha VRAM)

3. **VRAM Usage** (Uso de memoria GPU)
   - üéØ Objetivo: **<1GB**
   - ‚úÖ √ìptimo: 400-800MB
   - ‚ö†Ô∏è Evitar: >2GB (incompatible con pipeline serial)

4. **Inference Speed** (Velocidad de inferencia)
   - üéØ Objetivo: **>50 samples/sec** en GPU
   - ‚úÖ √ìptimo: 100-500 samples/sec
   - Buscar en model card: "throughput" o "samples/sec"

5. **Task Type**
   - ‚úÖ Usar: `zero-shot-classification` o `text-classification`
   - ‚úÖ Alternativa: `nli` (Natural Language Inference)
   - ‚ùå Evitar: `question-answering` (diferente prop√≥sito)

### Modelos Recomendados (Enero 2025)

#### Ultra-Ligero (GPU <4GB):
```yaml
classifier:
  class: "src.classifiers.transformer.TransformerClassifier"
  config:
    model: "prajjwal1/bert-tiny"
    # 4.4M params, ~200MB VRAM, 500+ samples/sec
```

#### Ligero (GPU 4-6GB):
```yaml
classifier:
  class: "src.classifiers.transformer.TransformerClassifier"
  config:
    model: "cross-encoder/nli-deberta-v3-xsmall"
    # 22M params, ~400MB VRAM, 200+ samples/sec
```

#### Balanceado (GPU 6-8GB) - **RECOMENDADO**:
```yaml
classifier:
  class: "src.classifiers.transformer.TransformerClassifier"
  config:
    model: "cross-encoder/nli-distilroberta-base"
    # 82M params, ~800MB VRAM, 100+ samples/sec
    # Excelente balance calidad/velocidad
```

#### Full (GPU >8GB):
```yaml
classifier:
  class: "src.classifiers.transformer.TransformerClassifier"
  config:
    model: "valhalla/distilbart-mnli-12-1"
    # 139M params, ~1.2GB VRAM, 50+ samples/sec
    # Mejor calidad pero m√°s lento
```

---

## Vectorizers (Embeddings)

### M√©tricas Clave:

1. **Embedding Dimension**
   - üéØ Objetivo: **384** (balance perfecto)
   - ‚úÖ Alternativa: 768 (m√°s calidad, m√°s VRAM)
   - ‚ö†Ô∏è Evitar: >1024 (overkill para chat history)

2. **Model Size**
   - üéØ Objetivo: **<500MB**
   - ‚úÖ √ìptimo: 100-400MB
   - ‚ö†Ô∏è Evitar: >1GB

3. **VRAM Usage**
   - üéØ Objetivo: **<1GB**
   - ‚úÖ √ìptimo: 400-800MB
   - Para calcular: ~= Model Size √ó 1.5

4. **MTEB Score** (Massive Text Embedding Benchmark)
   - üéØ Objetivo: **>55**
   - ‚úÖ √ìptimo: 58-65 (excelente para chat history)
   - üèÜ Top tier: >65 (para uso profesional)
   - Buscar en: https://huggingface.co/spaces/mteb/leaderboard

5. **Multilingual Support** (si necesitas espa√±ol)
   - ‚úÖ Buscar: "multilingual" o "multilang" en el nombre
   - ‚úÖ Verificar: languages en model card

### Modelos Recomendados (Enero 2025)

#### Ultra-Ligero (GPU <4GB):
```yaml
vectorizer:
  class: "src.vectorizers.sentence_transformer.SentenceTransformerVectorizer"
  config:
    model: "sentence-transformers/paraphrase-MiniLM-L3-v2"
    # Dim: 384, ~120MB VRAM, MTEB: 50.2
```

#### Ligero (GPU 4-6GB) - **ACTUALMENTE USAS ESTE** ‚úÖ:
```yaml
vectorizer:
  class: "src.vectorizers.sentence_transformer.SentenceTransformerVectorizer"
  config:
    model: "sentence-transformers/all-MiniLM-L6-v2"
    # Dim: 384, ~400MB VRAM, MTEB: 58.8
    # El mejor balance calidad/velocidad
```

#### Balanceado (GPU 6-8GB):
```yaml
vectorizer:
  class: "src.vectorizers.sentence_transformer.SentenceTransformerVectorizer"
  config:
    model: "BAAI/bge-small-en-v1.5"
    # Dim: 384, ~600MB VRAM, MTEB: 62.1
    # Mejor calidad que MiniLM
```

#### Multiling√ºe (Espa√±ol + English):
```yaml
vectorizer:
  class: "src.vectorizers.sentence_transformer.SentenceTransformerVectorizer"
  config:
    model: "intfloat/multilingual-e5-small"
    # Dim: 384, ~500MB VRAM, MTEB: 60.9
    # Excelente para ES+EN
```

#### Full (GPU >8GB):
```yaml
vectorizer:
  class: "src.vectorizers.sentence_transformer.SentenceTransformerVectorizer"
  config:
    model: "BAAI/bge-base-en-v1.5"
    # Dim: 768, ~1.1GB VRAM, MTEB: 63.6
```

---

## C√≥mo Evaluar en HuggingFace

### 1. Buscar en HuggingFace:
```
https://huggingface.co/models?pipeline_tag=sentence-similarity
Filter: Task = "Sentence Similarity"
Sort by: Downloads (m√°s populares = m√°s testeados)
```

### 2. Revisar Model Card:
- **"Model Details" section**: Buscar "Parameters" o "Size"
- **"Evaluation" section**: Buscar MTEB scores
- **"Environmental Impact" section**: A veces lista VRAM

### 3. Verificar en Papers/Blogs:
```
https://huggingface.co/spaces/mteb/leaderboard
```
Filtrar por:
- "Model Size" < 500MB
- "Avg" score > 55

### 4. Testear antes de producci√≥n:
```bash
# Probar con subset de 1000 mensajes
python drecall.py reindex --limit 1000

# Si funciona bien, hacer full reindex
python drecall.py reindex
```

---

## Recomendaci√≥n para tu RTX 2080 Super (8GB)

**Setup √ìptimo:**

```yaml
# config/optimized.yaml
components:
  classifier:
    class: "src.classifiers.transformer.TransformerClassifier"
    config:
      model: "cross-encoder/nli-distilroberta-base"
      # 82M params, 800MB VRAM, r√°pido
      batch_size: 32
      device: "cuda"

  vectorizer:
    class: "src.vectorizers.sentence_transformer.SentenceTransformerVectorizer"
    config:
      model: "BAAI/bge-small-en-v1.5"
      # Mejor que MiniLM, solo 200MB m√°s VRAM
      batch_size: 128
      device: "cuda"
```

**Uso de VRAM:**
- Classifier: ~800MB
- Vectorizer: ~600MB
- **Total secuencial**: <1.5GB (perfecto para 8GB)

---

## Cu√°ndo Usar Cada Profile

### `default.yaml` - M√°xima Calidad
- GPU >8GB o paciencia para esperar
- Transformer classifier + vectorizer
- Primera carga lenta, mejor accuracy

### `lite.yaml` - Balance ‚≠ê **RECOMENDADO**
- Cualquier GPU o CPU
- Rule-based classifier (instant√°neo) + vectorizer en GPU
- **Carga en <1 minuto, clasificaci√≥n funcional**

### `express.yaml` - Ultra-R√°pido
- Solo vectorizaci√≥n, sin classifier
- Ideal para datasets gigantes (>100k mensajes)
- Agregar classifier despu√©s: `drecall reindex`

---

## Switching Models (Sin Reimportar)

```bash
# 1. Cambiar modelo en config
vim config/default.yaml

# 2. Re-vectorizar (no reimporta, solo actualiza vectors)
python drecall.py reindex

# 3. Probar b√∫squeda
python drecall.py search "test query"
```

El sistema trackea qu√© vectorizer usaste en cada mensaje y solo re-vectoriza los necesarios.
