# config/lite.yaml
# Lightweight profile - Rule-based classifier, CPU refiner (slow but no GPU required)

components:
  format_detector:
    class: "src.detectors.multi_pattern.MultiPatternDetector"

  importers:
    - class: "src.importers.chatgpt.ChatGPTImporter"
      formats: ["chatgpt", "openai"]
      config: {}

  normalizer:
    class: "src.normalizers.markdown.MarkdownNormalizer"
    config:
      use_markitdown: true
      fallback_on_error: true
      preserve_code_blocks: true

  # LITE: Rule-based classifier (no GPU, instant loading)
  classifier:
    class: "src.classifiers.rule_based.RuleBasedClassifier"
    config: {}

  # LITE: Refiner on CPU (slow but works without GPU)
  # Comment out to disable refinement entirely
  # refiner:
  #   class: "src.refiners.llama_cpp.LlamaCppRefiner"
  #   config:
  #     model_path: "models/Qwen2.5-3B-Instruct-Q4_K_M.gguf"
  #     n_gpu_layers: 0  # CPU only
  #     n_ctx: 1024  # Reduced context for speed
  #     max_tokens: 150
  #     batch_size: 4

  # GTE vectorizer (better quality, GPU accelerated)
  vectorizer:
    class: "src.vectorizers.sentence_transformer.SentenceTransformerVectorizer"
    config:
      model: "Alibaba-NLP/gte-multilingual-base"
      batch_size: 128
      device: "cuda"

  storage:
    class: "src.storage.sqlite_vector.SQLiteVectorStorage"
    config:
      database: "data/recall.db"

pipeline:
  stages:
    - name: "import"
      components: ["format_detector", "importers"]

    - name: "normalize"
      components: ["normalizer"]
      gc_after: false

    - name: "classify"
      components: ["classifier"]
      gc_after: false  # Rule-based, no GPU to clear

    # Refine stage will be skipped if refiner not configured

    - name: "vectorize"
      components: ["vectorizer"]
      gc_after: true
      cuda_empty_cache: true

    - name: "persist"
      components: ["storage"]

memory:
  context_aware: true
  safety_margin_mb: 500
  force_gc_between_stages: true
  cuda_empty_cache_between_stages: true
  memory_profiling: true
