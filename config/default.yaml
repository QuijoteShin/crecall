# config/default.yaml
# Default configuration for ChatRecall with Semantic Refinement

# Component configuration (all swappable via interfaces)
components:
  # Format detection
  format_detector:
    class: "src.detectors.multi_pattern.MultiPatternDetector"

  # Importers (tried in order)
  importers:
    - class: "src.importers.chatgpt.ChatGPTImporter"
      formats: ["chatgpt", "openai"]
      config: {}

  # Content normalizer
  normalizer:
    class: "src.normalizers.markdown.MarkdownNormalizer"
    config:
      use_markitdown: true
      fallback_on_error: true
      preserve_code_blocks: true

  # Intent classifier - ELIMINADO: Qwen hace single-pass con intent+topics+is_question
  # classifier:
  #   class: "src.classifiers.transformer.TransformerClassifier"
  #   config:
  #     model: "valhalla/distilbart-mnli-12-1"
  #     batch_size: 32
  #     device: "cuda"

  # Semantic refiner (SLM for intent extraction)
  refiner:
    class: "src.refiners.llama_cpp.LlamaCppRefiner"
    config:
      model_path: "models/Qwen2.5-3B-Instruct-Q4_K_M.gguf"
      n_gpu_layers: -1  # -1 = all layers to GPU
      n_ctx: 2048
      max_tokens: 800
      batch_size: 8
      temperature: 0.1
      max_input_chars: 1500
      truncation_side: "right"  # "right" = corta final, "left" = corta inicio (preserva final)

  # Text vectorizer (GTE multilingual for better Spanish support)
  vectorizer:
    class: "src.vectorizers.sentence_transformer.SentenceTransformerVectorizer"
    config:
      model: "Alibaba-NLP/gte-multilingual-base"
      batch_size: 64
      device: "cuda"

  # Storage backend
  storage:
    class: "src.storage.sqlite_vector.SQLiteVectorStorage"
    config:
      database: "data/recall.db"

# Pipeline configuration
pipeline:
  # Sequential stages (executed in order)
  stages:
    - name: "import"
      components: ["format_detector", "importers"]

    - name: "normalize"
      components: ["normalizer"]
      gc_after: false

    # Stage classify ELIMINADO - Qwen hace single-pass

    - name: "refine"
      components: ["refiner"]
      gc_after: false  # Keep GPU warm for vectorizer
      cuda_empty_cache: false

    - name: "vectorize"
      components: ["vectorizer"]
      gc_after: true
      cuda_empty_cache: true

    - name: "persist"
      components: ["storage"]

# Memory management
memory:
  # Context-aware memory management
  context_aware: true

  # Safety margin in MB (reserved for system)
  safety_margin_mb: 500

  # Force garbage collection between stages
  force_gc_between_stages: true

  # Clear CUDA cache between GPU stages
  cuda_empty_cache_between_stages: true

  # Log memory usage
  memory_profiling: true
